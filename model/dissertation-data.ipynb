{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nhanes_data(data_dir):\n",
    "    file_list = [\n",
    "        ('2005-2006', 'DEMO_D'), ('2007-2008', 'DEMO_E'),  ('2009-2010', 'DEMO_F'),('2011-2012', 'DEMO_G'),  ('2013-2014', 'DEMO_H'),\n",
    "        ('2005-2006', 'BPX_D'), ('2007-2008', 'BPX_E'),  ('2009-2010', 'BPX_F'),('2011-2012', 'BPX_G'),  ('2013-2014', 'BPX_H'),\n",
    "        ('2005-2006', 'BMX_D'), ('2007-2008', 'BMX_E'),  ('2009-2010', 'BMX_F'),('2011-2012', 'BMX_G'),  ('2013-2014', 'BMX_H'),\n",
    "        ('2005-2006', 'SMQ_D'), ('2007-2008', 'SMQ_E'),  ('2009-2010', 'SMQ_F'),('2011-2012', 'SMQ_G'),  ('2013-2014', 'SMQ_H'),\n",
    "        ('2005-2006', 'DIQ_D'), ('2007-2008', 'DIQ_E'),  ('2009-2010', 'DIQ_F'),('2011-2012', 'DIQ_G'),  ('2013-2014', 'DIQ_H'),\n",
    "        ('2005-2006', 'CDQ_D'), ('2007-2008', 'CDQ_E'),  ('2009-2010', 'CDQ_F'),('2011-2012', 'CDQ_G'),  ('2013-2014', 'CDQ_H'),\n",
    "        ('2005-2006', 'TCHOL_D'), ('2007-2008', 'TCHOL_E'),  ('2009-2010', 'TCHOL_F'),('2011-2012', 'TCHOL_G'),  ('2013-2014', 'TCHOL_H'),\n",
    "    ]\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    for (year, data_file) in file_list:\n",
    "        sub_dir = os.path.join(data_dir, year)\n",
    "        if not os.path.exists(sub_dir):\n",
    "            os.makedirs(sub_dir)\n",
    "        url = 'http://wwwn.cdc.gov/Nchs/Nhanes/{0}/{1}.XPT'.format(year, data_file)\n",
    "        file_name = os.path.join(sub_dir, data_file + '.XPT')\n",
    "        if not os.path.exists(file_name):\n",
    "#             logging.info('Downloading: {}'.format(url))\n",
    "            urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the data and transfer into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nhanes_data(data_dir):\n",
    "\n",
    "    DEMO_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'DEMO_D.XPT'))\n",
    "    DEMO_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'DEMO_E.XPT'))\n",
    "    DEMO_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'DEMO_F.XPT'))\n",
    "    DEMO_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'DEMO_G.XPT'))\n",
    "    DEMO_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'DEMO_H.XPT'))\n",
    "    DEMO_cols = ['SEQN', 'RIDAGEYR', 'RIAGENDR']\n",
    "\n",
    "    BPX_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'BPX_D.XPT'))\n",
    "    BPX_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'BPX_E.XPT'))\n",
    "    BPX_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'BPX_F.XPT'))\n",
    "    BPX_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'BPX_G.XPT'))\n",
    "    BPX_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'BPX_H.XPT'))\n",
    "    BPX_cols = ['SEQN', 'BPXSY1', 'BPXDI1']\n",
    "\n",
    "    BMX_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'BMX_D.XPT'))\n",
    "    BMX_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'BMX_E.XPT'))\n",
    "    BMX_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'BMX_F.XPT'))\n",
    "    BMX_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'BMX_G.XPT'))\n",
    "    BMX_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'BMX_H.XPT'))\n",
    "    BMX_cols = ['SEQN', 'BMXBMI']\n",
    "\n",
    "    SMQ_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'SMQ_D.XPT'))\n",
    "    SMQ_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'SMQ_E.XPT'))\n",
    "    SMQ_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'SMQ_F.XPT'))\n",
    "    SMQ_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'SMQ_G.XPT'))\n",
    "    SMQ_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'SMQ_H.XPT'))\n",
    "    SMQ_cols = ['SEQN', 'SMQ020']\n",
    "\n",
    "    DIQ_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'DIQ_D.XPT'))\n",
    "    DIQ_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'DIQ_E.XPT'))\n",
    "    DIQ_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'DIQ_F.XPT'))\n",
    "    DIQ_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'DIQ_G.XPT'))\n",
    "    DIQ_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'DIQ_H.XPT'))\n",
    "    DIQ_cols = ['SEQN', 'DIQ010']\n",
    "    \n",
    "    TCHOL_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'TCHOL_D.XPT'))\n",
    "    TCHOL_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'TCHOL_E.XPT'))\n",
    "    TCHOL_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'TCHOL_F.XPT'))\n",
    "    TCHOL_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'TCHOL_G.XPT'))\n",
    "    TCHOL_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'TCHOL_H.XPT'))\n",
    "    TCHOL_cols = ['SEQN', 'LBXTC']\n",
    "    \n",
    "    CDQ_D = pd.read_sas(os.path.join(data_dir, '2005-2006', 'CDQ_D.XPT'))\n",
    "    CDQ_E = pd.read_sas(os.path.join(data_dir, '2007-2008', 'CDQ_E.XPT'))\n",
    "    CDQ_F = pd.read_sas(os.path.join(data_dir, '2009-2010', 'CDQ_F.XPT'))\n",
    "    CDQ_G = pd.read_sas(os.path.join(data_dir, '2011-2012', 'CDQ_G.XPT'))\n",
    "    CDQ_H = pd.read_sas(os.path.join(data_dir, '2013-2014', 'CDQ_H.XPT'))\n",
    "    CDQ_cols = ['SEQN', 'CDQ010']\n",
    "    \n",
    "\n",
    "    '''Merge Datasets\n",
    "    '''\n",
    "#     logging.info('Merging data...')\n",
    "    age = 40\n",
    "    df_00 = DEMO_D.loc[(DEMO_D.RIDAGEYR >= age),DEMO_cols] \\\n",
    "            .merge(BPX_D[BPX_cols], on='SEQN') \\\n",
    "            .merge(BMX_D[BMX_cols], on='SEQN') \\\n",
    "            .merge(SMQ_D[SMQ_cols], on='SEQN') \\\n",
    "            .merge(DIQ_D[DIQ_cols], on='SEQN') \\\n",
    "            .merge(TCHOL_D[TCHOL_cols], on='SEQN')\n",
    "\n",
    "    df_02 = DEMO_E.loc[(DEMO_E.RIDAGEYR >= age),DEMO_cols] \\\n",
    "            .merge(BPX_E[BPX_cols], on='SEQN') \\\n",
    "            .merge(BMX_E[BMX_cols], on='SEQN') \\\n",
    "            .merge(SMQ_E[SMQ_cols], on='SEQN') \\\n",
    "            .merge(DIQ_E[DIQ_cols], on='SEQN') \\\n",
    "            .merge(TCHOL_E[TCHOL_cols], on='SEQN')\n",
    "\n",
    "    df_04 = DEMO_F.loc[(DEMO_F.RIDAGEYR >= age),DEMO_cols] \\\n",
    "            .merge(BPX_F[BPX_cols], on='SEQN') \\\n",
    "            .merge(BMX_F[BMX_cols], on='SEQN') \\\n",
    "            .merge(SMQ_F[SMQ_cols], on='SEQN') \\\n",
    "            .merge(DIQ_F[DIQ_cols], on='SEQN') \\\n",
    "            .merge(TCHOL_F[TCHOL_cols], on='SEQN')\n",
    "    \n",
    "    df_06 = DEMO_G.loc[(DEMO_G.RIDAGEYR >= age),DEMO_cols] \\\n",
    "            .merge(BPX_G[BPX_cols], on='SEQN') \\\n",
    "            .merge(BMX_G[BMX_cols], on='SEQN') \\\n",
    "            .merge(SMQ_G[SMQ_cols], on='SEQN') \\\n",
    "            .merge(DIQ_G[DIQ_cols], on='SEQN') \\\n",
    "            .merge(TCHOL_G[TCHOL_cols], on='SEQN')\n",
    "    \n",
    "    df_08 = DEMO_H.loc[(DEMO_H.RIDAGEYR >= age),DEMO_cols] \\\n",
    "            .merge(BPX_H[BPX_cols], on='SEQN') \\\n",
    "            .merge(BMX_H[BMX_cols], on='SEQN') \\\n",
    "            .merge(SMQ_H[SMQ_cols], on='SEQN') \\\n",
    "            .merge(DIQ_H[DIQ_cols], on='SEQN') \\\n",
    "            .merge(TCHOL_H[TCHOL_cols], on='SEQN')\n",
    "    df_pop = pd.concat([df_00, df_02, df_04, df_06, df_08])\n",
    "\n",
    "\n",
    "    '''Diagnosed Cardiovascular '''\n",
    "    df_00_diag = df_00.merge(CDQ_D.loc[CDQ_D.CDQ010 == 1, CDQ_cols], on=\"SEQN\")\n",
    "    df_02_diag = df_02.merge(CDQ_E.loc[CDQ_E.CDQ010 == 1, CDQ_cols], on=\"SEQN\")\n",
    "    df_04_diag = df_04.merge(CDQ_F.loc[CDQ_F.CDQ010 == 1, CDQ_cols], on=\"SEQN\")\n",
    "    df_06_diag = df_06.merge(CDQ_G.loc[CDQ_G.CDQ010 == 1, CDQ_cols], on=\"SEQN\")\n",
    "    df_08_diag = df_08.merge(CDQ_H.loc[CDQ_H.CDQ010 == 1, CDQ_cols], on=\"SEQN\")\n",
    "    diag_total = pd.concat([df_00_diag, df_02_diag, df_04_diag, df_06_diag, df_08_diag])\n",
    "    diag_total.loc[:,'status'] = 1\n",
    "#     logging.info('Diagnosed subject count: {}'.format(diag_total.shape[0]))\n",
    "\n",
    "    '''No Cardiovascular'''\n",
    "    df_00_undiag = df_00.merge(CDQ_D.loc[CDQ_D.CDQ010 == 2, CDQ_cols], on='SEQN')\n",
    "    df_02_undiag = df_02.merge(CDQ_E.loc[CDQ_E.CDQ010 == 2, CDQ_cols], on='SEQN')\n",
    "    df_04_undiag = df_04.merge(CDQ_F.loc[CDQ_F.CDQ010 == 2, CDQ_cols], on='SEQN')\n",
    "    df_06_undiag = df_06.merge(CDQ_G.loc[CDQ_G.CDQ010 == 2, CDQ_cols], on='SEQN')\n",
    "    df_08_undiag = df_08.merge(CDQ_H.loc[CDQ_H.CDQ010 == 2, CDQ_cols], on='SEQN')\n",
    "    undiag_total = pd.concat([df_00_undiag, df_02_undiag, df_04_undiag, df_06_undiag, df_08_undiag])\n",
    "    undiag_total.loc[:,'status'] = 0\n",
    "#     logging.info('Undiagnosed subject count: {}'.format(undiag_total.shape[0]))\n",
    "\n",
    "    '''Join and split data'''\n",
    "    df = pd.concat([diag_total, undiag_total], ignore_index=True)\n",
    "    df = df.drop(['CDQ010'], axis=1)\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=289)\n",
    "\n",
    "    '''Save data'''\n",
    "    fname_train = os.path.join(data_dir, 'data_train_cvd.csv')\n",
    "    fname_test = os.path.join(data_dir, 'data_test_cvd.csv')\n",
    "    df_train.to_csv(fname_train, index=False, float_format='%.1f')\n",
    "#     logging.info('Training set saved: {}'.format(fname_train))\n",
    "    df_test.to_csv(fname_test, index=False, float_format='%.1f')\n",
    "#     logging.info('Test set saved: {}'.format(fname_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_dir = 'data'\n",
    "    download_nhanes_data(data_dir)\n",
    "    merge_nhanes_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
